{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.models import Model,Sequential\nfrom keras import optimizers\nfrom keras.layers import Input,Conv1D,BatchNormalization,MaxPooling1D,LSTM,Dense,Activation,Layer, Flatten\nimport keras.backend as K\nimport argparse\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom tensorflow import keras\nfrom matplotlib import pyplot as plt\nfrom IPython.display import clear_output\nimport tensorflow as tf\nfrom ppg_data_preprocessing import preprocess_according_paper2\n\nclass PlotLearning(keras.callbacks.Callback):\n    \"\"\"\n    Callback to plot the learning curves of the model during training.\n    \"\"\"\n    def on_train_begin(self, logs={}):\n        self.metrics = {}\n        for metric in logs:\n            self.metrics[metric] = []\n            \n\n    def on_epoch_end(self, epoch, logs={}):\n        # Storing metrics\n        for metric in logs:\n            if metric in self.metrics:\n                self.metrics[metric].append(logs.get(metric))\n            else:\n                self.metrics[metric] = [logs.get(metric)]\n        \n        # Plotting\n        metrics = [x for x in logs if 'val' not in x]\n        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n        clear_output(wait=True)\n\n        for i, metric in enumerate(metrics):\n            axs[i].plot(range(1, epoch + 2), \n                        self.metrics[metric], \n                        label=metric)\n            if logs['val_' + metric]:\n                axs[i].plot(range(1, epoch + 2), \n                            self.metrics['val_' + metric], \n                            label='val_' + metric)\n                \n            axs[i].legend()\n            axs[i].grid()\n\n        plt.tight_layout()\n        plt.show()\n        \ndef CnnTransformerModel():\n    i = Input(shape = (8*PPG_SAMPLING_RATE, 1))\n    \n    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(i)\n\n    x = Convolution1D(8, kernel_size = 10, strides = 10, activation='relu', padding = 'same')(x)\n    \n    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(x)\n    \n    x = Bidirectional(CuDNNLSTM(128, return_sequences = True, return_state = False))(x)\n    \n    x = Bidirectional(CuDNNLSTM(64, return_sequences = True, return_state = False))(x)\n    \n    avg_pool = GlobalAveragePooling1D()(x)\n    \n    avg_pool = Dense(60,activation = 'relu')(avg_pool)\n    \n    y = Dense(1,activation = 'relu')(avg_pool)\n    \n    return Model(inputs = [i], outputs = [y])\n\ndef model_compile():\n    model = CnnTransformerModel()\n    \n    model.compile(loss='mean_squared_error', optimizer= tf.keras.optimizers.RMSprop(\n        learning_rate=0.001,\n        rho=0.9,\n        momentum=0.0,\n        epsilon=1e-07,\n        centered=False,\n        name=\"RMSprop\"),metrics = ['mean_absolute_error'])\n    return model\n\ndef train(subject_no):\n    \n    model = model_compile()\n\n    model.compile(loss='mean_squared_error', optimizer= tf.keras.optimizers.RMSprop(\n        learning_rate=0.001,\n        rho=0.9,\n        momentum=0.0,\n        epsilon=1e-07,\n        centered=False,\n        name=\"RMSprop\"),metrics = ['mean_absolute_error'])\n   \n    (X_train, y_train), (X_valid, y_valid), (X_test, y_test) = preprocess_according_paper2(subject_no)\n    \n    nb_epochs = 100\n    batch_size = 256\n    weight_save_filename = \"weight_ECG_bilstm_subjectwise.h5\"\n\n    mdlcheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n        weight_save_filename)\n    callbacks_list = [PlotLearning(), mdlcheckpoint_cb]\n    model.fit(np.expand_dims(X_train, axis = 2),\n              y_train,\n              epochs = nb_epochs,\n              batch_size = batch_size,\n              validation_data=(np.expand_dims(X_valid, axis = 2), y_valid),\n              verbose=1,\n              shuffle=True,\n              callbacks=callbacks_list\n              )\n    y_pred = model.predict(np.expand_dims(X_test, axis = 2), batch_size = 256)\n    return mean_absolute_error(y_test, y_pred)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}